Goals for paper 2
    global fitting
        use continuous GA
            dominance sorting
            islands to promote diversitiy
            use/improve fitpy
                move fitpy to github
            monitor convergence (of each island & objective)
                visualize pca strengths for each objective
                    (equvalent to high dimensioanl sensitivity)

        replace session with model

        'parameter definition'
            describes limits/distribution for parameters
                distribution isn't strict, just for random mutations

        'parameter sets'
            values of variable parameters
            these are the values controlled by the GA/controller
            each parameter set is associated with a "job"
            this is the primary slicing table, columns like:
                par_set_id, par_A, par_B, ..., obj_A, obj_B, ...
                * unclear exactly how useful this is...probably prety useful
            additional results can be linked in other tables via par_set_id

        experiments
            have their own fixed values
            have their own parameter definitions
                which are grabbed by the controller and fed into the whole shebang

        'local blackboard' for experiment results
            to be used as inputs to further simulation/analysis
            this conveniently supports multi-stage sims like mitchison

        need input files that accomodate this
            validation code
            unittests for validation code might not be a bad idea
            have ./runtests.sh run input validation on all files

        need to be able to resume/calculate more points for a model

        able to add an objective to a model?
            * messy, but seemingly useful

    improve speed
        c++ implementation of filament
        consider c++ implementation of main loop

    local meshes
        start from previous best parameter set
        generate a local mesh to show that we are actually at a minimum


Simulation strategy
    cleanup/add numpy use where appropriate

    consider implementing loop in c++
        only after optimizing the filament itself

Postprocessing & data storage
    Floats
        D
        j (elongation rate)
        Critical Concentration

    Timecourses
        SEM
        Histograms

    State Distributions
        observe this & record in a meaningful way

    Objectives
        with/without Artbitrary normalization

    Slicing
        must slice against smallish number of parameters

Output
    Export data for plotting in external program
        include code version info

Database
    cascading deletes (important for maintaining clean db)
        special handling todo:
            if process is not attached to jobs, delete it
            delete slicing_* tables for deleted sessions.
        verify cascades work as expected
            can't this be done in unit tests?
